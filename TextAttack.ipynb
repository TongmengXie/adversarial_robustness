{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a551b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/tom/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow_hub) (2.1.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow_hub) (5.29.5)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow_hub) (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/miniconda3/envs/steering-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-16 14:28:03.749948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-16 14:28:04.000983: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-16 14:28:04.209464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752672484.338573    1310 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752672484.377496    1310 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752672484.745790    1310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752672484.745858    1310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752672484.745864    1310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752672484.745867    1310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-16 14:28:04.799282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at textattack/roberta-base-imdb were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-07-16 14:36:24.632561: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-16 14:36:35.428226: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "100%|██████████| 1/1 [08:55<00:00, 535.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1: 100%|██████████| 1/1 [09:00<00:00, 540.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 (100%)]] --> [[0 (58%)]]\n",
      "\n",
      "The [[movie]] was absolutely [[wonderful]]!\n",
      "\n",
      "The [[photographer]] was absolutely [[sumptuous]]!\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 1      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 40.0%  |\n",
      "| Average num. words per input: | 5.0    |\n",
      "| Avg num queries:              | 98.0   |\n",
      "+-------------------------------+--------+\n",
      "Original : <bound method AttackResult.original_text of <textattack.attack_results.successful_attack_result.SuccessfulAttackResult object at 0x75543b0db210>>\n",
      "Adversarial: <bound method AttackResult.perturbed_text of <textattack.attack_results.successful_attack_result.SuccessfulAttackResult object at 0x75543b0db210>>\n",
      "\n",
      "Detailed results:\n",
      "Original result: GoalFunctionResult( \n",
      "  (goal_function_result_type): Classification\n",
      "  (attacked_text): The movie was absolutely wonderful!\n",
      "  (ground_truth_output): 1\n",
      "  (model_output): 1\n",
      "  (score): 0.0010962486267089844\n",
      ")\n",
      "Perturbed result: GoalFunctionResult( \n",
      "  (goal_function_result_type): Classification\n",
      "  (attacked_text): The photographer was absolutely sumptuous!\n",
      "  (ground_truth_output): 1\n",
      "  (model_output): 0\n",
      "  (score): 0.5771352648735046\n",
      ")\n",
      "Number of queries: 98\n",
      "Goal function result: 1 (100%) --> 0 (58%)\n",
      "\n",
      "Prediction details:\n",
      "Original prediction: 1\n",
      "Adversarial prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import textattack\n",
    "except:\n",
    "    !pip install textattack\n",
    "\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.datasets import Dataset\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 1) Load model + tokenizer\n",
    "model_name = \"textattack/roberta-base-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model = model.to('cpu')\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# 2) Build the TextFooler attack\n",
    "attack = TextFoolerJin2019.build(model_wrapper)\n",
    "\n",
    "# 3) Create a tiny dataset with your sentence - Alternative approach\n",
    "# Create examples as dictionaries to be more explicit\n",
    "examples = [(\"The movie was absolutely wonderful!\", 1)]  \n",
    "\n",
    "# Create dataset with explicit input columns\n",
    "dataset = Dataset(examples, input_columns=[\"text\"])\n",
    "\n",
    "# 4) Run the attack\n",
    "attack_args = textattack.AttackArgs(num_examples=1, log_to_csv=None)\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "results = attacker.attack_dataset()\n",
    "\n",
    "# 5) Inspect the first result\n",
    "for res in results:\n",
    "    print(\"Original :\", res.original_text)\n",
    "    print(\"Adversarial:\", res.perturbed_text)\n",
    "    print(\"\\nDetailed results:\")\n",
    "    print(\"Original result:\", res.original_result)\n",
    "    print(\"Perturbed result:\", res.perturbed_result)\n",
    "    print(\"Number of queries:\", res.num_queries)\n",
    "    print(\"Goal function result:\", res.goal_function_result_str())\n",
    "    \n",
    "    # Additional debugging - show the actual predictions\n",
    "    print(\"\\nPrediction details:\")\n",
    "    print(\"Original prediction:\", res.original_result.output)\n",
    "    print(\"Adversarial prediction:\", res.perturbed_result.output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ee19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-imdb were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded!\n",
      "============================================================\n",
      "ADVERSARIAL ATTACK DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "\n",
      "2. TOXICITY DETECTION BYPASS\n",
      "----------------------------------------\n",
      "ORIGINAL: 'You are not intelligent and show no motivations.'\n",
      "ORIGINAL TOXICITY: toxic (0.524)\n",
      "\n",
      "BYPASS ATTEMPTS:\n",
      "✗ FAILED - punctuation: 'You a.re not intelligent and show no motivations.'\n",
      "  Toxicity: toxic (0.403)\n",
      "✗ FAILED - whitespace: 'You  are  not  intelligent  and  show  no  motivations.'\n",
      "  Toxicity: toxic (0.524)\n",
      "✗ FAILED - leetspeak: 'you 4re not 1ntell1gent 4nd show no mot1v4t1ons.'\n",
      "  Toxicity: toxic (0.627)\n",
      "✗ FAILED - unicode: 'you are not іntelligent and show no motivations.'\n",
      "  Toxicity: toxic (0.080)\n",
      "\n",
      "TOXICITY BYPASS SUCCESS RATE: 0/4 (0.0%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compact Adversarial Attack Demo\n",
    "Demonstrates model brittleness vs human perception with minimal code\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import textattack\n",
    "except ImportError:\n",
    "    !pip install textattack\n",
    "\n",
    "import re\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.datasets import Dataset\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.attack_args import AttackArgs\n",
    "from textattack.attacker import Attacker\n",
    "\n",
    "class CompactAdversarialDemo:\n",
    "    \"\"\"Minimal adversarial attack demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load sentiment and toxicity models\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        # Sentiment model\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-imdb\")\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-imdb\")\n",
    "        self.sentiment_wrapper = HuggingFaceModelWrapper(self.sentiment_model, self.sentiment_tokenizer)\n",
    "        \n",
    "        # Toxicity model\n",
    "        try:\n",
    "            self.toxicity_pipeline = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "        except:\n",
    "            self.toxicity_pipeline = pipeline(\"text-classification\", model=\"martin-ha/toxic-comment-model\")\n",
    "        \n",
    "        print(\"Models loaded!\")\n",
    "    \n",
    "    def attack_sentiment(self, text: str, label: int) -> Dict[str, Any]:\n",
    "        \"\"\"Attack sentiment classifier\"\"\"\n",
    "        dataset = Dataset([(text, label)], input_columns=[\"text\"])\n",
    "        attack = TextFoolerJin2019.build(self.sentiment_wrapper)\n",
    "        \n",
    "        attack_args = AttackArgs(num_examples=1, log_to_csv=None)\n",
    "        attacker = Attacker(attack, dataset, attack_args)\n",
    "        results = attacker.attack_dataset()\n",
    "        \n",
    "        if results:\n",
    "            result = results[0]\n",
    "            return {\n",
    "                'original': result.original_text,\n",
    "                'adversarial': result.perturbed_text,\n",
    "                'success': 'SUCCEEDED' in result.goal_function_result_str(),\n",
    "                'orig_pred': result.original_result.output,\n",
    "                'adv_pred': result.perturbed_result.output,\n",
    "                # 'human_readable': self.assess_human_perception(result.original_text, result.perturbed_text)\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def bypass_toxicity(self, toxic_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Try to bypass toxicity detection\"\"\"\n",
    "        original_score = self.check_toxicity(toxic_text)\n",
    "        \n",
    "        # Token-level attacks\n",
    "        attacks = {\n",
    "            'punctuation': self.inject_punctuation(toxic_text),\n",
    "            'whitespace': self.add_whitespace(toxic_text),\n",
    "            'leetspeak': self.to_leetspeak(toxic_text),\n",
    "            'unicode': self.substitute_unicode(toxic_text)\n",
    "        }\n",
    "        \n",
    "        results = {'original': original_score}\n",
    "        \n",
    "        for method, modified_text in attacks.items():\n",
    "            modified_score = self.check_toxicity(modified_text)\n",
    "            results[method] = {\n",
    "                'text': modified_text,\n",
    "                'bypassed': original_score['is_toxic'] and not modified_score['is_toxic'],\n",
    "                'score': modified_score\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def check_toxicity(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check if text is toxic\"\"\"\n",
    "        try:\n",
    "            result = self.toxicity_pipeline(text)[0]\n",
    "            return {\n",
    "                'is_toxic': result['label'].upper() in ['TOXIC', 'TOXICITY', '1'],\n",
    "                'confidence': result['score'],\n",
    "                'label': result['label']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'is_toxic': False, 'confidence': 0.0, 'label': 'ERROR', 'error': str(e)}\n",
    "    \n",
    "    def inject_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Add punctuation to bypass detection\"\"\"\n",
    "        words = text.split()\n",
    "        if words:\n",
    "            idx = random.randint(0, len(words) - 1)\n",
    "            word = words[idx]\n",
    "            if len(word) > 2:\n",
    "                pos = random.randint(1, len(word) - 1)\n",
    "                words[idx] = word[:pos] + '.' + word[pos:]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def add_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Add extra whitespace\"\"\"\n",
    "        return re.sub(r' ', '  ', text)\n",
    "    \n",
    "    def to_leetspeak(self, text: str) -> str:\n",
    "        \"\"\"Convert to leetspeak\"\"\"\n",
    "        leet_map = {'a': '4', 'e': '3', 'i': '1', 'o': '0', 's': '5'}\n",
    "        result = text.lower()\n",
    "        for char, replacement in leet_map.items():\n",
    "            if random.random() < 0.5:\n",
    "                result = result.replace(char, replacement)\n",
    "        return result\n",
    "    \n",
    "    def substitute_unicode(self, text: str) -> str:\n",
    "        \"\"\"Substitute with similar Unicode characters\"\"\"\n",
    "        subs = {'a': 'а', 'e': 'е', 'o': 'о', 'i': 'і'}  # Cyrillic lookalikes\n",
    "        result = text.lower()\n",
    "        for char, replacement in subs.items():\n",
    "            if char in result and random.random() < 0.3:\n",
    "                result = result.replace(char, replacement, 1)\n",
    "        return result\n",
    "    \n",
    "    def run_demo(self):\n",
    "        \"\"\"Run complete demonstration\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ADVERSARIAL ATTACK DEMONSTRATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 2. Toxicity Bypass Demo\n",
    "        print(\"\\n\\n2. TOXICITY DETECTION BYPASS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        toxic_text = \"You are not intelligent and show no motivations.\"\n",
    "        toxicity_results = self.bypass_toxicity(toxic_text)\n",
    "        \n",
    "        print(f\"ORIGINAL: '{toxic_text}'\")\n",
    "        print(f\"ORIGINAL TOXICITY: {toxicity_results['original']['label']} ({toxicity_results['original']['confidence']:.3f})\")\n",
    "        \n",
    "        print(\"\\nBYPASS ATTEMPTS:\")\n",
    "        for method, result in toxicity_results.items():\n",
    "            if method != 'original':\n",
    "                status = \"✓ SUCCESS\" if result['bypassed'] else \"✗ FAILED\"\n",
    "                print(f\"{status} - {method}: '{result['text']}'\")\n",
    "                print(f\"  Toxicity: {result['score']['label']} ({result['score']['confidence']:.3f})\")\n",
    "        \n",
    "        successful_bypasses = sum(1 for method, result in toxicity_results.items() \n",
    "                                if method != 'original' and result['bypassed'])\n",
    "        total_attempts = len(toxicity_results) - 1\n",
    "        \n",
    "        print(f\"\\nTOXICITY BYPASS SUCCESS RATE: {successful_bypasses}/{total_attempts} ({successful_bypasses/total_attempts*100:.1f}%)\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the compact demo\"\"\"\n",
    "    demo = CompactAdversarialDemo()\n",
    "    demo.run_demo()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steering-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
